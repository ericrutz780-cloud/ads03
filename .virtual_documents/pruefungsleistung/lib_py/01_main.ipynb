


import os
from datetime import datetime
import pandas as pd
import requests
from bs4 import BeautifulSoup
import bs4
import numpy as np

import time
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from selenium import webdriver





STORAGE_DIR = "../input"


OUTPUT_DIR = "../output"


DOC_DIR = "../docu"





# List of news pages to be scraped
newspaper_urls = dict(
    sz="https://www.sueddeutsche.de/thema/Nahost",
    zeit="https://www.zeit.de/thema/israel",
    spiegel="https://www.spiegel.de/thema/nahost/p2/",
)





content_dict = {}
text_dict = {}
log_list = []
failing_list = []





# Current date as string
now = datetime.now()
now_str = now.strftime("%Y-%m-%d")
print(now_str)








def scrape_website(name, url):
    # (1) Run request
    response = requests.get(url, allow_redirects=True)
    content = response.content
    text = response.text

    # (2) File name to store the raw HTML
    file_name = os.path.join(
        STORAGE_DIR,
        f"{now_str}-{name}.html",
    )

    # (3) Write raw HTML
    with open(file_name, "wb") as f:
        f.write(response.content)

    # (4) Fill content_dict and text_dict
    content_dict[name] = response.content
    text_dict[name] = response.text

    # (5) Fill log_list
    log_info = dict(
        name=name,
        date=now_str,
        file_name=file_name,
        status=response.status_code,
        url=response.url,
        encoding=response.encoding,
    )
    log_list.append(log_info)





for name, url in newspaper_urls.items():
    try:
        scrape_website(name, url)
    except:
        failing_list.append((name, url))





sz="https://www.sueddeutsche.de/thema/Nahost"


sz_html = os.path.join("../input/2025-11-20-sz.html")


with open(sz_html, "r", encoding= "utf-8") as f:
    sz_text = f.read()





sz_soup = BeautifulSoup(sz_text, "html.parser")


sz_article_tag = sz_soup.select('article.sz-teaserlist-element')


article_id = []
title = []
content_preview = []
datum = []

for article in sz_article_tag:
    
    id_tag = article.attrs.get('id', 'KEINE ID GEFUNDEN')
    article_id.append(id_tag)

    #Titel des Artikels
#------------------------------------------------------------------------------------------------------    
    title_tag = article.select('[data-manual="teaser-title"]')

    if title_tag:

        for t in title_tag:
        
            title.append(t.get_text(strip=True))
                    
    else: 
        title.append('NA')

    # Vorschau des Artikel Inhaltes
#------------------------------------------------------------------------------------------------------    
    preview_tag = article.select('[data-manual="teaser-text"]')

    if preview_tag:
        for p in preview_tag:
        
            content_preview.append(p.get_text(strip=True))

    else:
        content_preview.append('NA')

    # Datum der Veröffentlichung
#------------------------------------------------------------------------------------------------------    
    datum_tag = article.select('span.css-0')

    
    if datum_tag:

        for d in datum_tag:
            
            datum.append(d.get_text(strip=True))

    else:
        datum.append('NA')

print(f"Gefundene Artikel: {len(article_id)}") # Geändert
print(title)
print(content_preview)
print(datum)


article_dict = {
    'id': article_id,
    'title': title,
    'preview': content_preview,
    'date': datum
}


sz_df = pd.DataFrame(article_dict)
sz_df





sz = "https://www.sueddeutsche.de/thema/Nahost"
target_date = datetime.strptime("08.10.2025", "%d.%m.%Y")
name = "sz"

driver = webdriver.Chrome()
driver.get(sz)

try:
    wait = WebDriverWait(driver, 10)
    cookie_button = wait.until(
        EC.element_to_be_clickable((By.ID, "sdd-cmp-modal-button-agree")) 
    )
    cookie_button.click()
    time.sleep(1) 
except TimeoutException:
    pass

date_selector = (By.CSS_SELECTOR, 'span.css-0')

while True:
    try:
        count_before = len(driver.find_elements(*date_selector))
    
        wait = WebDriverWait(driver, 10)
        load_element = wait.until(
            EC.presence_of_element_located((By.CLASS_NAME, "css-1kk8krp"))
        )

        driver.execute_script("arguments[0].scrollIntoView(true);", load_element)
        time.sleep(0.5) 

        driver.execute_script("arguments[0].click();", load_element)

        try:
            wait.until(
                lambda d: len(d.find_elements(*date_selector)) > count_before
            )
        except TimeoutException:
            break 
            
        all_dates = driver.find_elements(*date_selector)
        if not all_dates:
            break

        last_date_text = all_dates[-1].text.strip()

        try:
            current_date = datetime.strptime(last_date_text, "%d.%m.%Y")
            if current_date < target_date:
                break
        except ValueError: 
            pass 

    except TimeoutException:
        break

rohes_html = driver.page_source

driver.quit()

file_name = os.path.join(
    STORAGE_DIR,
    f"{now_str}-{name}.html",
)

try:
    with open(file_name, "w", encoding="utf-8") as f:
        f.write(rohes_html)
    print(f"Erfolgreich gespeichert in: {file_name}")

except Exception as e:
    print(f"Fehler beim Speichern der Datei: {e}")





zeit = "https://www.zeit.de/thema/israel"


zeit_html = os.path.join("../input/2025-11-21-zeit.html")


with open(zeit_html, "r", encoding= "utf-8") as f:
    zeit_text = f.read()


zeit_driver = webdriver.Chrome()





zeit_soup = BeautifulSoup(zeit_text, "html.parser")


zeit_article_tag = zeit_soup.select('article.zon-teaser.zon-teaser--standard.kilkaya-all')


zeit_article_id = []
zeit_title = []
zeit_content_preview = []
zeit_datum = []

for zeit_article in zeit_article_tag:
    
    #id_tag = article.attrs.get('id', 'KEINE ID GEFUNDEN')
    #article_id.append(id_tag)

    #Titel des Artikels
#------------------------------------------------------------------------------------------------------    
    zeit_title_tag = zeit_article.select('span.zon-teaser__title')

    if zeit_title_tag:

        for t in zeit_title_tag:
        
            zeit_title.append(t.get_text(strip=True))
                    
    else: 
        zeit_title.append('NA')

    # Vorschau des Artikel Inhaltes
#------------------------------------------------------------------------------------------------------    
    zeit_preview_tag = zeit_article.select('p.zon-teaser__summary')

    if zeit_preview_tag:
        for p in zeit_preview_tag:
        
            zeit_content_preview.append(p.get_text(strip=True))

    else:
        zeit_content_preview.append('NA')

    # Datum der Veröffentlichung
#------------------------------------------------------------------------------------------------------    
    zeit_datum_tag = zeit_article.select('time.zon-teaser__datetime')

    
    if zeit_datum_tag:

        for d in zeit_datum_tag:
            
            zeit_datum.append(d.get_text(strip=True))

    else:
        zeit_datum.append('NA')

#print(f"Gefundene Artikel: {len(article_id)}") # Geändert
print(zeit_title)
print(zeit_content_preview)
print(zeit_datum)


zeit_article_dict = {
    #'id': zeit_article_id,
    'title': zeit_title,
    'preview': zeit_content_preview,
    'date': zeit_datum
}


zeit_df = pd.DataFrame(zeit_article_dict)
zeit_df





# 1. EINSTELLUNGEN
start_url = "https://www.zeit.de/thema/nahost"
# Wir suchen Artikel, die ÄLTER sind als dieser Tag (also bis zum 7.10. und davor)
target_date = datetime(2025, 10, 8) 

driver = webdriver.Chrome()
driver.get(start_url)

# 2. COOKIE BANNER (Versuch, ihn wegzuklicken)
try:
    wait = WebDriverWait(driver, 5)
    # Sucht nach "Zustimmen" oder "Akzeptieren"
    cookie_btn = wait.until(EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'Zustimmen') or contains(text(), 'Akzeptieren')]")))
    cookie_btn.click()
    time.sleep(1)
    print("Cookie Banner geklickt.")
except:
    print("Kein Cookie Banner gefunden oder schon weg.")

all_articles = []
page_num = 1
stop_found = False

while True:
    print(f"--- Verarbeite Seite {page_num} ---")
    
    # HTML holen und Parsen
    soup = BeautifulSoup(driver.page_source, "html.parser")
    articles_on_page = soup.select('article.zon-teaser')
    
    if not articles_on_page:
        print("Keine Artikel auf dieser Seite gefunden.")
        break
        
    print(f"Gefundene Artikel auf dieser Seite: {len(articles_on_page)}")

    for art in articles_on_page:
        # Titel holen
        title_node = art.select_one('.zon-teaser__title')
        title = title_node.get_text(strip=True) if title_node else "NA"
        
        # Datum holen (aus dem datetime Attribut!)
        time_node = art.select_one('time.zon-teaser__datetime')
        
        date_obj = None
        date_str = "NA"
        
        if time_node and time_node.has_attr('datetime'):
            try:
                # Format ist meistens ISO: '2025-10-07T21:46:32+02:00'
                # Wir schneiden alles nach dem 'T' ab für den Vergleich
                date_str = time_node['datetime']
                iso_date_part = date_str.split('T')[0] 
                date_obj = datetime.strptime(iso_date_part, "%Y-%m-%d")
            except Exception as e:
                # Falls das Datum komisch formatiert ist, überspringen wir diesen einen Artikel
                continue
        
        # Zur Liste hinzufügen
        all_articles.append([title, date_str])
        
        # PRÜFUNG: Haben wir das Ziel erreicht?
        if date_obj and date_obj < target_date:
            print(f"!!! STOPP-DATUM ERREICHT: {date_obj.strftime('%d.%m.%Y')} bei Artikel: '{title}'")
            stop_found = True
            break 
    
    if stop_found:
        break

    # --- NÄCHSTE SEITE LADEN ---
    try:
        # Versuche den "Nächste Seite" Button zu finden
        # Strategie 1: CSS Klasse (Standard bei Zeit)
        next_btn = driver.find_element(By.CSS_SELECTOR, "a.pager__button--next")
        
        # Scrollen, damit der Button nicht verdeckt ist
        driver.execute_script("arguments[0].scrollIntoView(true);", next_btn)
        time.sleep(1)
        
        # Klicken
        next_btn.click()
        
        # Warten, bis die URL sich ändert (z.B. auf ?p=2)
        page_num += 1
        WebDriverWait(driver, 10).until(EC.url_contains(f"p={page_num}"))
        
        # Kurze Pause für den Aufbau der Seite
        time.sleep(2)
        
    except (NoSuchElementException, TimeoutException):
        print("Kein 'Nächste Seite'-Button mehr gefunden oder Ende erreicht.")
        break

driver.quit()

# ERGEBNIS
print(f"\nGesamt gefundene Artikel: {len(all_articles)}")
if len(all_articles) > 0:
    print("Letzter gefundener Artikel:")
    print(all_articles[-1])





spiegel="https://www.spiegel.de/thema/nahost/p2/"


spiegel_html = os.path.join("../input/2025-11-07-spiegel.html")


with open(spiegel_html, "r", encoding= "utf-8") as f:
    spiegel_text = f.read()


spiegel_soup = BeautifulSoup(spiegel_text, "html.parser")





# Large

driver.get(spiegel)

time.sleep(3) 


spiegel_html = driver.page_source
spiegel_soup = BeautifulSoup(spiegel_html, 'lxml')

spiegel_title_tag = spiegel_soup.select('span.align-middle')

print(spiegel_title_tag)


title_attr_df(spiegel_title_tag)





spiegel_content_preview_tag = spiegel_soup.find_all('span', attrs={'data-target-teaser-el': True})


title_attr_df(spiegel_content_preview_tag)





spiegel_date_tag = spiegel_soup.find_all('span', attrs={'data-auxiliary': True})


title_attr_df(spiegel_date_tag)








sz_df


from selenium import webdriver
from selenium.webdriver.common.by import By

from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

from selenium.common.exceptions import TimeoutException, StaleElementReferenceException
import time

from selenium.webdriver.support.ui import WebDriverWait


driver = webdriver.Chrome()
driver.get(sz)


load_element = driver.find_element(By.CLASS_NAME, 'css-1kk8krp')


# --- Dein Setup (Beispiel) ---
now_str = datetime.now().strftime("%Y-%m-%d")
name = "sz"

driver = webdriver.Chrome()
driver.get(sz)

#--------------------Laden der Seite--------------------
target_date = datetime.strptime("08.10.2025", "%d.%m.%Y")
print(f"Suche gestartet. Stoppe, wenn Datum älter als {target_date.strftime('%d.%m.%Y')} ist.")

while True:
    try:
        wait = WebDriverWait(driver, 10)
        load_element = wait.until(
            EC.element_to_be_clickable((By.CLASS_NAME, "css-1kk8krp"))
        )
        load_element.click()

        time.sleep(2) 

        all_dates = driver.find_elements(By.CSS_SELECTOR, 'span.css-0')
        if not all_dates:
            print("Keine Datumsangaben mehr gefunden.")
            break

        last_date_text = all_dates[-1].text.strip()

        try:
            current_date = datetime.strptime(last_date_text, "%d.%m.%Y")
            if current_date < target_date:
                print(f"STOPP: {last_date_text} ist älter als das Ziel.")
                break
        except ValueError:
            pass 

    except TimeoutException:
        print("Button nicht mehr gefunden. Ende erreicht.")
        break


##--------------------Verlängerte Seite speichern--------------------

rohes_html = driver.page_source

driver.quit()

file_name = os.path.join(
    STORAGE_DIR,
    f"{now_str}-{name}.html",
)

try:
    with open(file_name, "w", encoding="utf-8") as f:
        f.write(rohes_html)
    print(f"Erfolgreich gespeichert in: {file_name}")

except Exception as e:
    print(f"Fehler beim Speichern der Datei: {e}")


driver = webdriver.Chrome()
driver.get(sz)


driver = webdriver.Chrome()
driver.get(sz)

button_element = driver.find_element(By.CLASS_NAME, 'css-1kk8krp')

datum_tag2 = article.select('span.css-0')
    
if datum_tag:

    for d in datum_tag:
            
        datum.append(d.get_text(strip=True))

else:
    datum2.append('NA')

for date in datum_tag2:
    date != "08.10.2025"
    button_element.click()

try:
    wait = WebDriverWait(driver, 10)
    load_element = wait.until(
        EC.element_to_be_clickable(load_element)
    )
except TimeoutException:
    print("Fehler: Button 'Mehr Artikel laden' wurde nach 10 Sek. nicht klickbar.")


load_element.click()


sz = "https://www.sueddeutsche.de/thema/Nahost"
target_date = datetime.strptime("08.10.2025", "%d.%m.%Y")
name = "sz"

driver = webdriver.Chrome()
driver.get(sz)

try:
    wait = WebDriverWait(driver, 10)
    cookie_button = wait.until(
        EC.element_to_be_clickable((By.ID, "sdd-cmp-modal-button-agree")) 
    )
    cookie_button.click()
    time.sleep(1) 
except TimeoutException:
    pass

date_selector = (By.CSS_SELECTOR, 'span.css-0')

while True:
    try:
        count_before = len(driver.find_elements(*date_selector))
    
        wait = WebDriverWait(driver, 10)
        load_element = wait.until(
            EC.presence_of_element_located((By.CLASS_NAME, "css-1kk8krp"))
        )

        driver.execute_script("arguments[0].scrollIntoView(true);", load_element)
        time.sleep(0.5) 

        driver.execute_script("arguments[0].click();", load_element)

        try:
            wait.until(
                lambda d: len(d.find_elements(*date_selector)) > count_before
            )
        except TimeoutException:
            break 
            
        all_dates = driver.find_elements(*date_selector)
        if not all_dates:
            break

        last_date_text = all_dates[-1].text.strip()

        try:
            current_date = datetime.strptime(last_date_text, "%d.%m.%Y")
            if current_date < target_date:
                break
        except ValueError: 
            pass 

    except TimeoutException:
        break

rohes_html = driver.page_source

driver.quit()

file_name = os.path.join(
    STORAGE_DIR,
    f"{now_str}-{name}.html",
)

try:
    with open(file_name, "w", encoding="utf-8") as f:
        f.write(rohes_html)
    print(f"Erfolgreich gespeichert in: {file_name}")

except Exception as e:
    print(f"Fehler beim Speichern der Datei: {e}")



