


import os
from datetime import datetime
import pandas as pd
import requests
from bs4 import BeautifulSoup
import bs4
import numpy as np

import time
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from selenium import webdriver





STORAGE_DIR = "../input"


OUTPUT_DIR = "../output"


DOC_DIR = "../docu"





# List of news pages to be scraped
newspaper_urls = dict(
    sz="https://www.sueddeutsche.de/thema/Nahost",
    zeit="https://www.zeit.de/thema/israel",
    spiegel="https://www.spiegel.de/thema/nahost/p2/",
)





content_dict = {}
text_dict = {}
log_list = []
failing_list = []





# Current date as string
now = datetime.now()
now_str = now.strftime("%Y-%m-%d")
print(now_str)








def scrape_website(name, url):
    # (1) Run request
    response = requests.get(url, allow_redirects=True)
    content = response.content
    text = response.text

    # (2) File name to store the raw HTML
    file_name = os.path.join(
        STORAGE_DIR,
        f"{now_str}-{name}.html",
    )

    # (3) Write raw HTML
    with open(file_name, "wb") as f:
        f.write(response.content)

    # (4) Fill content_dict and text_dict
    content_dict[name] = response.content
    text_dict[name] = response.text

    # (5) Fill log_list
    log_info = dict(
        name=name,
        date=now_str,
        file_name=file_name,
        status=response.status_code,
        url=response.url,
        encoding=response.encoding,
    )
    log_list.append(log_info)





for name, url in newspaper_urls.items():
    try:
        scrape_website(name, url)
    except:
        failing_list.append((name, url))








sz="https://www.sueddeutsche.de/thema/Nahost"


sz_html = os.path.join("../input/2025-11-14-sz.html")


with open(sz_html, "r", encoding= "utf-8") as f:
    sz_text = f.read()





sz_soup = BeautifulSoup(sz_text, "html.parser")


sz_article_tag = sz_soup.select('article.sz-teaserlist-element.css-epgeur')


article_id = []
title = []
content_preview = []
datum = []

for article in sz_article_tag:
    
    id_tag = article.attrs.get('id', 'KEINE ID GEFUNDEN')
    article_id.append(id_tag)

    #Titel des Artikels
#------------------------------------------------------------------------------------------------------    
    title_tag = article.select('span.css-ornnzr')

    if title_tag:

        for t in title_tag:
        
            title.append(t.get_text(strip=True))
                    
    else: 
        title.append('NA')

    # Vorschau des Artikel Inhaltes
#------------------------------------------------------------------------------------------------------    
    preview_tag = article.select('p.css-a6ecqs')

    if preview_tag:
        for p in preview_tag:
        
            content_preview.append(p.get_text(strip=True))

    else:
        content_preview.append('NA')

    # Datum der Veröffentlichung
#------------------------------------------------------------------------------------------------------    
    datum_tag = article.select('span.css-0')

    
    if datum_tag:

        for d in datum_tag:
            
            datum.append(d.get_text(strip=True))

    else:
        datum.append('NA')

print(f"Gefundene Artikel: {len(article_id)}") # Geändert
print(title)
print(content_preview)
print(datum)





# Geändert: 'ValueError' aus diesem Import entfernt
import time
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException # Geändert

# Setup Selenium
now_str = datetime.now().strftime("%Y-%m-%d")
name = "sz"

driver = webdriver.Chrome()
driver.get(sz)

# Cookie-Banner-Selektor
try:
    wait = WebDriverWait(driver, 10)
    # Versuch, den Button über seine eindeutige ID zu finden
    cookie_button = wait.until(
        EC.element_to_be_clickable((By.ID, "sdd-cmp-modal-button-agree")) 
    )
    cookie_button.click()
    print("Cookie-Banner geklickt.")
    time.sleep(1) 
except TimeoutException:
    print("Kein Cookie-Banner gefunden (oder er war nicht klickbar). Fahre fort.")

#--------------------Laden der Seite--------------------+
target_date = datetime.strptime("08.10.2025", "%d.%m.%Y")
print(f"Suche gestartet. Stoppe, wenn Datum älter als {target_date.strftime('%d.%m.%Y')} ist.")

# Geändert: Wir definieren den Datums-Selektor für die Warte-Logik
date_selector = (By.CSS_SELECTOR, 'span.css-0')

while True:
    try:
        # Geändert: Zähle, wie viele Datums-Elemente BEREITS da sind
        count_before = len(driver.find_elements(*date_selector))
    
        wait = WebDriverWait(driver, 10)
        load_element = wait.until(
            EC.presence_of_element_located((By.CLASS_NAME, "css-1kk8krp"))
        )

        driver.execute_script("arguments[0].scrollIntoView(true);", load_element)
        time.sleep(0.5) 

        driver.execute_script("arguments[0].click();", load_element)

        # Geändert: Warte, bis die ANZAHL DER DATEN zugenommen hat
        try:
            wait.until(
                lambda d: len(d.find_elements(*date_selector)) > count_before
            )
            print(f"Datumsanzahl erhöht auf: {len(driver.find_elements(*date_selector))}")
        except TimeoutException:
            print("Wartezeit überschritten, aber es wurden keine neuen Datums-Elemente geladen.")
            break 
            
        # Geändert: Datumsprüfung (jetzt sicher, da Daten geladen sind)
        all_dates = driver.find_elements(*date_selector) # Geändert: Verwende den Selektor
        if not all_dates:
            print("Keine Datumsangaben mehr gefunden.")
            break

        last_date_text = all_dates[-1].text.strip()

        try:
            current_date = datetime.strptime(last_date_text, "%d.%m.%Y")
            if current_date < target_date:
                print(f"STOPP: {last_date_text} ist älter als das Ziel.")
                break
        except ValueError: 
            pass 

    except TimeoutException:
        print("Button nicht mehr gefunden. Ende erreicht.")
        break


##--------------------Verlängerte Seite speichern--------------------

rohes_html = driver.page_source

driver.quit()

file_name = os.path.join(
    STORAGE_DIR,
    f"{now_str}-{name}.html",
)

try:
    with open(file_name, "w", encoding="utf-8") as f:
        f.write(rohes_html)
    print(f"Erfolgreich gespeichert in: {file_name}")

except Exception as e:
    print(f"Fehler beim Speichern der Datei: {e}")


# Die Variable 'rohes_html' wird direkt verwendet,
# Datei zu laden.
sz_soup = BeautifulSoup(rohes_html, "html.parser")

sz_article_tag = sz_soup.select('article.sz-teaserlist-element.css-epgeur')

#-----------Seite Parsen------------

article_id = []
title = []
content_preview = []
datum = []

for article in sz_article_tag:
    
    id_tag = article.attrs.get('id', 'KEINE ID GEFUNDEN')
    article_id.append(id_tag)

    #Titel des Artikels
#------------------------------------------------------------------------------------------------------    
    title_tag = article.select('span.css-ornnzr')

    if title_tag:

        for t in title_tag:
        
            title.append(t.get_text(strip=True))
                    
    else: 
        title.append('NA')

    # Vorschau des Artikel Inhaltes
#------------------------------------------------------------------------------------------------------    
    preview_tag = article.select('p.css-a6ecqs')

    if preview_tag:
        for p in preview_tag:
        
            content_preview.append(p.get_text(strip=True))

    else:
        content_preview.append('NA')

    # Datum der Veröffentlichung
#------------------------------------------------------------------------------------------------------    
    datum_tag = article.select('span.css-0')

    
    if datum_tag:

        for d in datum_tag:
            
            datum.append(d.get_text(strip=True))

    else:
        datum.append('NA')

print(f"Gefundene Artikel: {len(article_id)}") # Geändert
print(title)
print(content_preview)
print(datum)


article_dict = {
    'id': article_id,
    'title': title,
    'preview': content_preview,
    'date': datum
}


sz_df = pd.DataFrame(article_dict)
sz_df





zeit = "https://www.zeit.de/thema/israel"


zeit_html = os.path.join("../input/2025-11-07-zeit.html")


with open(zeit_html, "r", encoding= "utf-8") as f:
    zeit_text = f.read()


driver = webdriver.Chrome()





# Extra Large

driver.get(zeit)

time.sleep(3) 


html = driver.page_source
soup = BeautifulSoup(html, 'lxml')

zeit_title_tag = soup.select('span.zon-teaser__title.zon-teaser__title--extralarge')
zeit_title_tag2 = soup.select('span.zon-teaser__title.zon-teaser__title--large')
zeit_title_tag3 = soup.select('span.zon-teaser__title')

print(zeit_title_tag)
print(zeit_title_tag2)
print(zeit_title_tag3)


title_attr_df(zeit_title_tag)





# Large

driver.get(zeit)

time.sleep(3) 


s_zeit_html = driver.page_source
zeit_soup = BeautifulSoup(html, 'lxml')

zeit_content_preview_tag = soup.select('p.zon-teaser__summary')

print(zeit_content_preview_tag)


title_attr_df(zeit_content_preview_tag)





spiegel_date_tag = spiegel_soup.find_all('time', attrs={'datetime': True})


# Large

driver.get(zeit)

time.sleep(3) 


s_zeit_html = driver.page_source
zeit_soup = BeautifulSoup(html, 'lxml')

zeit_date_tag = soup.select('time.zon-teaser__datetime')

print(zeit_date_tag)


title_attr_df(zeit_date_tag)





faz="https://www.faz.net/aktuell/politik/thema/nahostkonflikt"


faz_html = os.path.join("../input/2025-11-07-faz.html")


with open(faz_html, "r", encoding= "utf-8") as f:
    faz_text = f.read()


faz_title_tag = soup.find_all('h3')


title_attr_df(faz_title_tag)


driver.get(faz)

time.sleep(3) 


s_faz_html = driver.page_source
faz_soup = BeautifulSoup(html, 'lxml')

faz_title_tag = soup.select('h3.text-22.font-serif')

print(faz_title_tag)


driver.get(faz)

time.sleep(3) 


s_faz_html = driver.page_source
faz_soup = BeautifulSoup(html, 'lxml')

faz_title_tag = soup.select('h3.item')

print(faz_title_tag)





az_soup.select('h3.text-22.font-serif')








ts="https://www.tagesspiegel.de/internationales/themen/krieg-in-nahost"


ts_html = os.path.join("../input/2025-11-07-ts.html")


with open(ts_html, "r", encoding= "utf-8") as f:
    ts_text = f.read()


ts_soup = BeautifulSoup(ts_text, "html.parser")


ts_title_tag = ts_soup.find_all('span', attrs={'data-kilkaya': True})





title_attr_df(ts_title_tag)





ts_content_preview_tag = ts_soup.find_all('p', attrs={'data-kilkaya': True})


title_attr_df(ts_content_preview_tag)





ts_date_tag = ts_soup.find_all('span', attrs={'data-kilkaya': True})











spiegel="https://www.spiegel.de/thema/nahost/p2/"


spiegel_html = os.path.join("../input/2025-11-07-spiegel.html")


with open(spiegel_html, "r", encoding= "utf-8") as f:
    spiegel_text = f.read()


spiegel_soup = BeautifulSoup(spiegel_text, "html.parser")





# Large

driver.get(spiegel)

time.sleep(3) 


spiegel_html = driver.page_source
spiegel_soup = BeautifulSoup(spiegel_html, 'lxml')

spiegel_title_tag = spiegel_soup.select('span.align-middle')

print(spiegel_title_tag)


title_attr_df(spiegel_title_tag)





spiegel_content_preview_tag = spiegel_soup.find_all('span', attrs={'data-target-teaser-el': True})


title_attr_df(spiegel_content_preview_tag)





spiegel_date_tag = spiegel_soup.find_all('span', attrs={'data-auxiliary': True})


title_attr_df(spiegel_date_tag)








sz_df


from selenium import webdriver
from selenium.webdriver.common.by import By

from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

from selenium.common.exceptions import TimeoutException, StaleElementReferenceException
import time

from selenium.webdriver.support.ui import WebDriverWait


driver = webdriver.Chrome()
driver.get(sz)


load_element = driver.find_element(By.CLASS_NAME, 'css-1kk8krp')


# --- Dein Setup (Beispiel) ---
now_str = datetime.now().strftime("%Y-%m-%d")
name = "sz"

driver = webdriver.Chrome()
driver.get(sz)

#--------------------Laden der Seite--------------------
target_date = datetime.strptime("08.10.2025", "%d.%m.%Y")
print(f"Suche gestartet. Stoppe, wenn Datum älter als {target_date.strftime('%d.%m.%Y')} ist.")

while True:
    try:
        wait = WebDriverWait(driver, 10)
        load_element = wait.until(
            EC.element_to_be_clickable((By.CLASS_NAME, "css-1kk8krp"))
        )
        load_element.click()

        time.sleep(2) 

        all_dates = driver.find_elements(By.CSS_SELECTOR, 'span.css-0')
        if not all_dates:
            print("Keine Datumsangaben mehr gefunden.")
            break

        last_date_text = all_dates[-1].text.strip()

        try:
            current_date = datetime.strptime(last_date_text, "%d.%m.%Y")
            if current_date < target_date:
                print(f"STOPP: {last_date_text} ist älter als das Ziel.")
                break
        except ValueError:
            pass 

    except TimeoutException:
        print("Button nicht mehr gefunden. Ende erreicht.")
        break


##--------------------Verlängerte Seite speichern--------------------

rohes_html = driver.page_source

driver.quit()

file_name = os.path.join(
    STORAGE_DIR,
    f"{now_str}-{name}.html",
)

try:
    with open(file_name, "w", encoding="utf-8") as f:
        f.write(rohes_html)
    print(f"Erfolgreich gespeichert in: {file_name}")

except Exception as e:
    print(f"Fehler beim Speichern der Datei: {e}")





driver = webdriver.Chrome()
driver.get(sz)

button_element = driver.find_element(By.CLASS_NAME, 'css-1kk8krp')

datum_tag2 = article.select('span.css-0')
    
if datum_tag:

    for d in datum_tag:
            
        datum.append(d.get_text(strip=True))

else:
    datum2.append('NA')

for date in datum_tag2:
    date != "08.10.2025"
    button_element.click()

try:
    wait = WebDriverWait(driver, 10)
    load_element = wait.until(
        EC.element_to_be_clickable(load_element)
    )
except TimeoutException:
    print("Fehler: Button 'Mehr Artikel laden' wurde nach 10 Sek. nicht klickbar.")


load_element.click()



